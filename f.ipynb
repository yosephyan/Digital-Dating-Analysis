{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98f73b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"lemmatized_messages.csv\")\n",
    "\n",
    "convo_user = df[['id', 'match_id', 'averageConversationLength', 'lemmatized_message']]\n",
    "\n",
    "convo_user.to_csv('finale.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a03286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the file\n",
    "file_path = 'news.csv'  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to extract relevant data\n",
    "def extract_relevant_data(conversations_str):\n",
    "    try:\n",
    "        conversations = json.loads(conversations_str)\n",
    "        relevant_data = []\n",
    "        for convo in conversations:\n",
    "            match_id = convo.get('match_id', '')\n",
    "            for msg in convo.get('messages', []):\n",
    "                message = msg.get('message', '')\n",
    "                relevant_data.append({'match_id': match_id, 'message': message})\n",
    "        return relevant_data\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "# Apply the function\n",
    "df['extracted_data'] = df['conversations'].apply(extract_relevant_data)\n",
    "\n",
    "# Explode the dataframe\n",
    "exploded_df = df.explode('extracted_data')\n",
    "\n",
    "# Extract 'match_id' and 'message'\n",
    "exploded_df['match_id'] = exploded_df['extracted_data'].apply(lambda x: x.get('match_id') if isinstance(x, dict) else None)\n",
    "exploded_df['message'] = exploded_df['extracted_data'].apply(lambda x: x.get('message') if isinstance(x, dict) else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "final_df = exploded_df.drop(columns=['conversations', 'extracted_data'])\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_file_path = 'newss.csv'  # Replace with your desired output path\n",
    "final_df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86af1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j_/xgvzbrt119v6lnhbyn1b46940000gn/T/ipykernel_26154/3426602523.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file_path_1)\n"
     ]
    }
   ],
   "source": [
    "# Let's try reading and merging the files again after the code execution state reset.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the files\n",
    "file_path_1 = 'newss.csv'\n",
    "file_path_2 = 'new.csv'\n",
    "\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Combining the files using 'id' as a reference\n",
    "combined_df = pd.merge(df1, df2, on='id')\n",
    "combined_df.to_csv('newsss.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e8891d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yosephyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yosephyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/yosephyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to convert NLTK's part of speech tags to wordnet's format\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('finalclean_noemoji.csv')\n",
    "\n",
    "# Function to lemmatize a sentence\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Lemmatize each word with its correct part-of-speech tag\n",
    "    return ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words])\n",
    "\n",
    "# Clean and lemmatize the 'message' column\n",
    "df['lemmatized_message'] = df['message'].apply(lambda x: lemmatize_sentence(re.sub(r'\\W+', ' ', x.lower())))\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('lemmatized_messages.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63e9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Training Accuracy: 0.7820147979510529 Development Accuracy: 0.7463986675069776\n",
      "Naive Bayes - Training Accuracy: 0.7506825000241166 Development Accuracy: 0.7321508958314576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yosephyan/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'finale.csv'  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a Bag of Words model with a threshold of 3\n",
    "vectorizer = CountVectorizer(min_df=5)\n",
    "X = vectorizer.fit_transform(df['message'])\n",
    "\n",
    "# Create a binary target variable: 1 if above average, 0 if below\n",
    "average_length = df['averageConversationLength'].mean()\n",
    "y = (df['averageConversationLength'] > average_length).astype(int)\n",
    "\n",
    "# Split the dataset into training and development datasets\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg_train_accuracy = accuracy_score(y_train, log_reg.predict(X_train))\n",
    "log_reg_dev_accuracy = accuracy_score(y_dev, log_reg.predict(X_dev))\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_train_accuracy = accuracy_score(y_train, nb_model.predict(X_train))\n",
    "nb_dev_accuracy = accuracy_score(y_dev, nb_model.predict(X_dev))\n",
    "\n",
    "\"\"\"\n",
    "# Multilayer Perceptron (MLP)\n",
    "mlp_model = MLPClassifier(max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_train_accuracy = accuracy_score(y_train, mlp_model.predict(X_train))\n",
    "mlp_dev_accuracy = accuracy_score(y_dev, mlp_model.predict(X_dev))\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_model.predict(X_train))\n",
    "rf_dev_accuracy = accuracy_score(y_dev, rf_model.predict(X_dev))\"\"\"\n",
    "\n",
    "# Print out the accuracies for each model\n",
    "print(\"Logistic Regression - Training Accuracy:\", log_reg_train_accuracy, \"Development Accuracy:\", log_reg_dev_accuracy)\n",
    "print(\"Naive Bayes - Training Accuracy:\", nb_train_accuracy, \"Development Accuracy:\", nb_dev_accuracy)\n",
    "#print(\"Multilayer Perceptron - Training Accuracy:\", mlp_train_accuracy, \"Development Accuracy:\", mlp_dev_accuracy)\n",
    "#print(\"Random Forest - Training Accuracy:\", rf_train_accuracy, \"Development Accuracy:\", rf_dev_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b17fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
